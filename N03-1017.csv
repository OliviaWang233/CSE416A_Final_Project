a common approach to phrase-based translation is to extract an inventory of phrase pairs (ppi) from bitext (koehn et al, 2003), for example, in the phraseextract algorithm (och, 2002), a word alignment ˆam1 is generated over the bitext, and all word subsequences ei2i1 and fj2j1 are found that satisfy : ˆam1 : ˆaj ∈ [i1,i2] iff j ∈ [j1,j2] . 
we generated for each phrase pair in the translation table 5 features: phrase translation probability (both directions), lexical weighting (koehn et al, 2003) (both directions) and phrase penalty (constant value). 
the lexical-weighting features are estimated using a method similar to that of koehn et al (2003). 
training begins with phrase pairs, obtained as by och, koehn, and others: giza++ (och and ney, 2000) is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the final-and� method of koehn et al (2003); then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. 
(och and ney, 2004; koehnetal.,2003;marcuandwong,2002))charac
the basic model uses the following features, analogous to pharaoh’s default feature set: • p(γ | α) and p(α | γ) • the lexical weights pw(γ | α) and pw(α | γ) (koehn et al, 2003);1 • a phrase penalty exp(1); • a word penalty exp(l), where l is the number of terminals in α. 
in (koehn et al, 2003), various aspects of phrase-based systems are compared.
although the best performing systems are “phrasebased” (see, for instance, och and ney (2004) or koehn et al (2003)), possible phrase translations must first be extracted from word-aligned bilingual text segments. 
the phrase-based decoder extracts phrases from the word alignments produced by giza++, and computes translation probabilities based on the frequency of one phrase being aligned with another (koehn et al, 2003). 
the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to (koehn et al, 2003), but the lexical translation probability a27 a14a12a94 a29 a97a100a21 is derived from the block set itself rather than from a word alignment, resulting in a simplified training. 
the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to (koehn et al, 2003), details are given in section 3.4. 
this section describes a phrase-based model for smt similar to the models presented in (koehn et al, 2003; och et al, 1999; tillmann and xia, 2003). 
two block sets are derived for each of the training sets using a phrase-pair selection algorithm similar to (koehn et al, 2003; tillmann and xia, 2003). 
by introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e∗ = argmaxe pr(e | f) = argmaxe summationdisplay a pr(e,a | f) ≈ argmaxe,a pr(e,a | f) exploiting the maximum entropy (berger et al, 1996) framework, the conditional distribution pr(e,a | f) can be determined through suitable real valued functions (called features) hr(e,f,a),r = 1...r, and takes the parametric form: pλ(e,a | f) ∝ exp{ rsummationdisplay r=1 λrhr(e,f,a)} the itc-irst system (chen et al, 2005) is based on a log-linear model which extends the original ibm model 4 (brown et al, 1993) to phrases (koehn et al, 2003; federico and bertoldi, 2005). 
our method for identifying paraphrases is an extension of recent work in phrase-based statistical machine translation (koehn et al, 2003). 
one is distortion model (och and ney, 2004; koehn et al, 2003) which penalizes translations according to their jump distance instead of their content. 
statistical phrase-based translation (koehn et al, 2003): here “phrase-based” means “subsequence-based”, as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases. 
recent work in statistical machine translation (mt) has sought to overcome the limitations of phrase-based models (marcu and wong, 2002; koehn et al, 2003; och and ney, 2004) by making use of syntactic information. 
word alignment and phrase extraction we used the giza++ word alignment software 3 to produce initial word alignments for our miniature bilingual corpus consisting of the source french file and the english reference file, and the refined word alignment strategy of (och and ney, 2003; koehn et al, 2003; tiedemann, 2004) to obtain improved word and phrase alignments. 
similarly, (koehn et al, 2003) propose a relative distortion model to be used with a phrase decoder. 
(koehn et al, 2003) used the following distortion model, which simply penalizes non-monotonic phrase alignments based on the word distance of successively translated source phrases with an appropriate value for the parameter.
figure 4: n-best phrase alignments phrase pairs are extracted using the standard phrase extraction method described in (koehn et al, 2003). 
standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (koehn et al, 2003; och and ney, 2004). 
here, ppicker� shows the accuracy when phrases are extracted by using the n-best phrase alignment method described in section 4.1, while growdiag-final� shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in (koehn et al, 2003). 
the translation model used in (koehn et al, 2003) is the product of translation probability a34a35a4 a29 a0 a33 a6 a29 a2 a33 a8 and distortion probability a36a37a4a39a38 a33a41a40a43a42a44a33a46a45 a32 a8 , a3a5a4a35a29 a0 a30 a32 a6 a29 a2 a30 a32 a8 a10 a30 a47 a33a49a48 a32 a34a35a4 a29 a0a22a33 a6 a29 a2 a33a50a8 a36a51a4a39a38 a33 a40a52a42 a33a53a45 a32 a8 (1) where a38 a33 denotes the start position of the source phrase translated into the a54 -th target phrase, and a42 a33a53a45 a32 denotes the end position of the source phrase translated into the a4a53a54 a40a56a55 a8 -th target phrase. 
during the last few years, smt systems have evolved from the original word-based approach (brown et al, 1993) to phrase-based translation systems (koehn et al, 2003). 
the process of phrase extraction is difficult to optimize in a non-discriminative setting: many heuristics have been proposed (koehn et al, 2003), but it is not obvious which one should be chosen for a given language pair. 
in the future, we plan to explore our discriminative framework on a full distortion model (koehn et al, 2003) or even a hierarchical model (chiang, 2005). 
the heuristics in koehn et al (2003) decide whether to extract a given phrase pair based on the underlying word alignments (see figure 3 for three examples), which we call constellations. 
the proposed system is phrase-based, as in koehn et al (2003), but uses an online perceptron training scheme to learn model parameters. 
to facilitate comparison with previous work, we created the translation tables using the same techniques as koehn et al (2003).3 
we used sentences of length 5-15 to facilitate comparisons with koehn et al (2003) and to enable rapid experimentation with various feature sets. 
at the end we ran our models once on test to get final numbers.2 4 models our experiments used phrase-based models (koehn et al, 2003), which require a translation table and language model for decoding and feature computation. 
we also compared our model with pharaoh (koehn et al, 2003). 
the phrase-based model of koehn et al (2003) is an instance of this framework. 
koehn et al (2003) demonstrated that choosing the appropriate heuristic for extracting phrases is very important. 
the statistical components of our system are modeled on the phrase-based system of koehn et al (2003), and component weights are adjusted by minimum error rate training (och, 2003). 
final results are reported on the test set of 1,755 sentences of length 5-15 that was used in koehn et al (2003). 
under the nist measure, we achieve results in the range of the state-of-the-art phrase-based system of koehn et al (2003) for in-coverage examples of the lfgbased system. 
recent approaches to statistical machine translation (smt) piggyback on the central concepts of phrasebased smt (och et al, 1999; koehn et al, 2003) and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process. 
in an experimental evaluation on the test-set that was used in koehn et al (2003) we show that for examples that are in coverage of the grammar-based system, we can achieve stateof-the-art quality on n-gram based evaluation measures. 
the statistical components of our system are modeled on the statistical components of the phrase-based system pharaoh, described in koehn et al (2003) and koehn (2004). 
phrase-based smt systems have been shown to outperform word-based approaches (koehn et al, 2003). 
a phrase-based translation model can be estimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted (koehn et al, 2003). 
today's statistical machine translation systems rely on high quality phrase translation pairs to acquire state-of-the-art performance, see (koehn et al, 2003; zens and ney, 2004; och and ney, 2003). 
we collect bidirectional (bi) refined word alignment by growing the intersection of chinese-to-english (ce) alignments and english-to-chinese (ec) alignments with the neighboring unaligned word pairs which appear in the union similar to the final-and� approaches (koehn, 2003; och and ney, 2003; tillmann, 2003). 
currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time [och, 2002; zens et al, 2002; koehn et al, 2003; vogel et al, 2003; tillmann, 2003] phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus. 
we carry out experiments using a phrase-based statistical machine translation system [koehn et al, 2003; koehn, 2004]. 
during the last decade, statistical machine translation (smt) systems have evolved from the original word-based approach (brown et al, 1993) into phrase-based translation systems (koehn et al, 2003). 
in recent years, phrase-based systems for statistical machine translation (och et al, 1999; koehn et al, 2003; venugopal et al, 2003) have delivered state-of-the-art performance on standard translation tasks. 
we use the model of koehn et al (2003) as a baseline for our experiments. 
evaluation in terms of both bleu scores and human judgments shows that our system performs similarly to the phrase-based model of koehn et al (2003). 
in this paper, we implement the translation of modifiers (step 3) with the phrase-based system of koehn et al (2003). 
each list contained the n-best translations produced by the phrase-based system of koehn et al (2003). 
in this paper, we use the phrase-based system of koehn et al (2003) to generate n-best translations for each of the modifiers, and we then use a discriminative reranking algorithm (bartlett et al, 2004) to choose between these modifiers. 
an evaluation of the method on translation from german to english shows similar performance to the phrase-based model of koehn et al (2003). 
modifiers within german clauses were translated using the phrase-based model of koehn et al (2003). 
for example, koehn et al (2003) reported that requiring constituents to be syntactically motivated does not lead to better constituent pairs, but only fewer constituent pairs, with loss of a good amount of valuable knowledge.
is relevant to finite-state phrase-based models that use no parse trees (koehn et al, 2003), tree-tostring models that rely on one parse tree (yamada and knight, 2001), and tree-to-tree models that rely on two parse trees (groves et al, 2004, e.g.). 
for maximum phrase length, we used 3 (based on what was suggested by (koehn etal., 2003)and 7(the default maximum phrase length in pharaoh). 
we therefore adopted a modified weighting scheme following(koehnetal., 2003), whichincorporates null alignments. 
different approaches have been suggested as using relative frequencies (zens et al 2002), calculate probabilities based on a statistical word-to-word dictionary (vogel et al 2003) or use a linear interpolation of these scores (koehn et al 2003). 
the advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (koehn et al 2003; vogel et al 2003; zens et al 2002; marcu and wong, 2002). 
more recently, phrase-based models (och et al, 1999; marcu and wong, 2002; koehn et al, 2003) have been proposed as a highly successful alternative to the ibm models. 
results using the method show an improvement from 25.2% bleu score to 26.8% bleu score (a statistically significant improvement), using a phrase-based system (koehn et al, 2003) which has been shown in the past to be a highly competitive smt system. 
recent research on statistical machine translation (smt) has lead to the development of phrase-based systems (och et al, 1999; marcu and wong, 2002; koehn et al, 2003). 
in experiments with the system of (koehn et al, 2003) we have found that in practice a large number of complete translations are completely monotonic (i.e., have a0 skips), suggesting that the system has difficulty learning exactly what points in the translation should allow reordering. 
in this paper we use the phrase-based system of (koehn et al, 2003) as our underlying model. 
our baseline is the phrase-based mt system of (koehn et al, 2003). 
under a phrase based translation model (koehn et al, 2003; marcu and wong, 2002), this distinction is important and will be discussed in more detail. 
the first system is the pharaoh decoder provided by (koehn et al, 2003) for the shared data task. 
for further information on these parameter settings, confer (koehn et al, 2003). 
in recent years, various phrase translation approaches (marcu and wong, 2002; och et al, 1999; koehn et al, 2003) have been shown to outperform word-to-word translation models (brown et al, 1993). 
the huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (koehn et al, 2003) as the probability of reappearance of larger phrases decreases. 
it has the advantage of naturally capturing local reorderings and is shown to outperform word-based machine translation (koehn et al, 2003). 
we collected the pp parameters bys imply reading the alignment matrices resulting from the word alignment, in a way similar to the one described in (koehn et al, 2003). 
this includes the standard notion of phrase, popular with phrasedbased smt (koehn et al, 2003; vogel et al, 2003) aswellassequencesofwordsthatcontaingaps(possibly of arbitrary size). 
accordingly, in this section we describe a set of experiments which extends the work of (way and gough, 2005) by evaluating the marker-based ebmt system of (gough & way, 2004b) against a phrase-based smt system built using the following components: • giza++, to extract the word-level correspondences; • the giza++ word alignments are then refined and used to extract phrasal alignments ((och & ney, 2003); or (koehn et al, 2003) for a more recent implementation); • probabilities of the extracted phrases are calculated from relative frequencies; • the resulting phrase translation table is passed to the pharaoh phrase-based smt decoder which along with sri language modelling toolkit5 performs translation. 
this translation model differs from the well known phrase-based translation approach (koehn et al, 2003) in two basic issues: rst, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. 
phrase-pairs are then extracted from the word alignments (koehn et al, 2003). 
the second one is heuristic and tries to use a word-aligned corpus (zens et al, 2002; koehn et al, 2003). 
we generate widl-expressions from chinese strings by exploiting a phrase-based translation table (koehn et al, 2003). 
a similar argument applies to phrase-based translation methods (e.g., koehn et al (2003)). 
see (och and ney, 2000), (yamada and knight, 2001), (koehn and knight, 2002), (koehn et al, 2003), (schafer and yarowsky, 2003) and (gildea, 2003). 
nowadays, most of the state-of-the-art smt systems are based on bilingual phrases (bertoldi et al, 2004; koehn et al, 2003; och and ney, 2004; tillmann, 2003; vogel et al, 2004; zens and ney, 2004). 
the focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided — for more on phrase-based statistical machine translation, refer to koehn et al (2003). 
in this project we use the model described by koehn et al (2003) which extracts its phrase alignments from a corpus that has been word aligned. 
on smaller data sets (koehn et al, 2003) the joint model shows performance comparable to the standard model, however the joint model does not reach the level of performance of the stan156 en-es es-en joint 3-gram, dl4 20.51 26.64 5-gram, dl6 26.34 27.17 + lex. 
most phrase-based translation models (och, 2003; koehn et al, 2003; vogel et al, 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. 
the impact of constraining the joint model trained on 10,000 sentences of the german-english europarl corpora and tested with the europarl test set used in koehn et al (2003) than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. 
for the future, the joint model would benefit from lexical weighting like that used in the standard model (koehn et al, 2003). 
we use the following features for our rules: • sourceand target-conditioned neg-log lexical weights as described in (koehn et al, 2003b) • neg-log relative frequencies: left-handside-conditioned, target-phrase-conditioned, source-phrase-conditioned • counters: n.o. 
baseline pharaoh with phrases extracted from ibm model 4 training with maximum phrase length 7 and extraction method diag-growthfinal (koehn et al, 2003a) lex phrase-decoder simulation: using only the initial lexical rules from the phrase table, all with lhs x, the glue rule, and a binary reordering rule with its own reordering-feature • xcat all nonterminals merged into a single x nonterminal: simulation of the system hiero (chiang, 2005). 
recent work in machine translation has evolved from the traditional word (brown et al, 1993) and phrase based (koehn et al, 2003a) models to include hierarchical phrase models (chiang, 2005) and bilingual synchronous grammars (melamed, 2004). 
we present results that compare our system against the baseline pharaoh implementation (koehn et al, 2003a) and mer training scripts provided for this workshop. 
the hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (koehn et al, 2003a). 
138 2 rule generation we start with phrase translations on the parallel training data using the techniques and implementation described in (koehn et al, 2003a). 
to evaluate neuralign, we used giza++ in both directions (e-to-f and f-to-e, where f is either chinese (c) or spanish (s)) as input and a refined alignment approach (och and ney, 2000) that uses a heuristic combination method called grow-diagfinal (koehn et al, 2003) for comparison. 
2 the problem of coverage in smt statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation (marcu and wong, 2002; koehn et al, 2003; och and ney, 2004). 
for comparison purposes, three additional heuristically-induced alignments are generated for each system: (1) intersection of both directions (aligner(int)); (2) union of both directions (aligner(union)); and (3) the previously bestknown heuristic combination approach called growdiag-final (koehn et al, 2003) (aligner(gdf)). 
during decoding, the number of english phrases per fl phrase was limited to 100 and the distortion of phrases was limited by 4. based on the observations in (koehn et al, 2003), we also limited the phrase length to 3 for computational reasons. 
word alignment—detection of corresponding words between two sentences that are translations of each other—is usually an intermediate step of statistical machine translation (mt) (brown et al, 1993; och and ney, 2003; koehn et al, 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. 
limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (niessen and ney, (2004), lee,(2004), yamada and knight (2001), marcu and wong (2002), och and ney (2004),koehn et al (2003), among others.) 
we proceeded with the following sequence of experiments: (1) baseline: as a baseline system, we used a pure word-based approach and used pharaoh training tool (2004), to train on the 22,500 sentences, and decoded using pharaoh (koehn et al, 2003) to obtain translations for a test set of 50 sentences. 
791 and score the alignment template model phrases (koehn et al, 2003). 
the features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (koehn et al, 2003); phrase translation model probabilities; and trigram language model probabilities logp(t), using kneser-ney smoothing as implemented in the srilm toolkit (stolcke, 2002). 
traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities (koehn et al, 2003), eg, p(˜s|˜t) = c(˜s,˜t)/summationtext˜s c(˜s,˜t) (since the estimation problems for p(˜s|˜t) and p(˜t|˜s) are symmetrical, we will usually refer only to p(˜s|˜t) for brevity). 
to derive the joint counts c(˜s,˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (koehn et al, 2003), with symmetrized word alignments generated using ibm model 2 (brown et al, 1993). 
this is the traditional approach for glass-box smoothing (koehn et al, 2003; zens and ney, 2004). 
koehn et al (2003) compare a number of different approaches to phrase-based statistical machine 255 length num uniq (mil) average # translations avg trans length 1 .88 
1while the improvements to translation quality reported in koehn et al (2003) are minor, their evaluation metric may not have been especially sensitive to adding longer phrases. 
table 4 gives statistics about phrases which occur more than once in the english section of the europarl corpus (koehn, 2002) which was used in the koehn et al (2003) experiments. 
the framework that we used to calculate the translation probabilities was similar to that detailed in koehn et al (2003). 
based on their analysis of the relationship between translation quality and phrase length, koehn et al (2003) suggest limiting phrase length to three words or less. 
our system is a re-implementation of the phrase-based system described in koehn (2003), and uses publicly available components for word alignment (och and ney, 2003)1, decoding (koehn, 2004a)2, language modeling (stolcke, 2002)3 and finite-state processing (knight and al-onaizan, 1999)4. 
we used the word alignment produced by giza (och and ney, 2000) out of an ibm model 2. we did try to use the alignment produced with ibm model 4, but did not notice significant differences over our experiments; an observation consistent with the findings of koehn et al (2003). 
for acquiring a pbm, we followed the approach described by koehn et al (2003). 
(koehn et al, 2003) show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only. 
the pharaoh decoder (koehn, 2003) use an alternative 66 figure 2: flow chart associated to the expansion of a hypothesis when using a multi-stack algorithm. 
on the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (yamada and knight, 2001) , alignment templates are used in (och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (pbt) in (marcu and wong, 2002; zens et al, 2002; koehn et al, 2003; tom´as and casacuberta, 2003). 
koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. 
the system follows the structure proposed in the documentation for the pharaoh decoder and uses many publicly available components (koehn, 2003b). 
pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models (koehn, 2003b). 
while the model and training regimen for φem differ from the model from marcu and wong (2002), we achieved results similar to koehn et al (2003a): φem slightly underperformed φh. 
we view this as a particularly promising aspect of our work, given that phrase-based systems such as pharaoh (koehn et al, 2003) perform better with higher recall alignments. 
most current smt systems (och and ney, 2004; koehn et al, 2003) use a generative model for word alignment such as the freely available giza++ (och and ney, 2003), an implementation of the ibm alignment models (brown et al, 1993). 
for an initial alignment, we used giza++ in both directions (e-to-f and f-to-e, where f is either chinese (c) or spanish (s)), and also two different combined alignments: intersection of e-to-f and f-to-e; and ra using a heuristic combination approach called grow-diag-final (koehn et al, 2003). 
the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (och and ney, 2000; koehn et al, 2003)—henceforth referred to as “ra.” 
for our experiments, we chose giza++ (och and ney, 2000) and the ra approach (koehn et al, 2003)— the best known alignment combination technique— as our initial aligners.1 4.2 tbl templates our templates consider consecutive words (of size 1, 2 or 3) in both languages. 
in englishto-german, this result produces results very comparable to a phrasal smt system (koehn et al, 2003) trained on the same data. 
it has been shown that phrasal machine translation systems are not affected by the quality of the input word alignments (koehn et al, 2003). 
this dependency graph is partitioned into treelets; like (koehn et al, 2003), we assume a uniform probability distribution over all partitions. 
for each differently tokenized corpus, we computed word alignments by a hmm translation model (och and ney, 2003) and by a word alignment refinement heuristic of “grow-diagfinal” (koehn et al, 2003). 
our phrase-based model uses a standard pharaoh feature functions listed as follows (koehn et al, 2003): • relative-count based phrase translation probabilities in both directions. 
one is a phrase-based translation in which a phrasal unit is employed for translation (koehn et al, 2003). 
second, phrase translation pairs are extracted from the word aligned corpus (koehn et al, 2003). 
 the phrase extraction algorithm is based on those presented by koehn et al (2003). 
in a phrase-based statistical translation (koehn et al, 2003), a bilingual text is decomposed as k phrase translation pairs (¯e1, ¯f¯a1), (¯e2, ¯f¯a2 ),...: 
the decoding process is very similar to those described in (koehn et al, 2003): it starts from an initial empty hypothesis. 
for details, please refer to koehn et al (2003). 
as an additional baseline, we compare against a phrasal smt decoder, pharaoh (koehn et al 2003). 
we used the heuristic combination described in (och and ney 2003) and extracted phrasal translation pairs from this combined alignment as described in (koehn et al, 2003). 
systems must therefore impose some limits on phrasal reordering, often hard limits based on distance as in koehn et al (2003) or some linguistically motivated constraint, such as itg (zens and ney, 2004). 
the last several years have seen phrasal statistical machine translation (smt) systems outperform word-based approaches by a wide margin (koehn 2003). 
details of this model are described by koehn et al (2003). 
defining scms the work presented here was done in the context of phrase-based mt (koehn et al, 2003; och and ney, 2004). 
phrase tables were learned from the training corpus using the diag-and� method (koehn et al, 2003), and using ibm model 2 to produce initial word alignments (these authors found this worked as well as ibm4). 
a comparison of the two approaches can be found in koehn, och, and marcu (2003). 
however, (koehn et al 2003) found that it is actually harmful to restrict phrases to constituents in parse trees, because the restriction would cause the system to miss many reliable translations, such as the correspondence between “there is” in english and “es gibt” (“it gives”) in german. 
the normalization is visualized as a translation problem where messages in the sms language are to be translated to normal english using a similar phrase-based statistical mt method (koehn et al, 2003). 
for our experiments we used the following features, analogous to pharaoh's default feature set: • p(γ | α) and p(α | γ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (och and ney, 2002);  the lexical weights  and (koehn et al, 2003), which estimate how well the words in α translate the words in a phrase penalty exp(1), which allows the model to learn a preference for longer or shorter derivations, analogous to koehn's phrase penalty (koehn, 2003). 
we compared a baseline system, the state-of-the-art phrase-based system pharaoh (koehn et al, 2003; koehn, 2004a), against our system. 
we obtain the word alignments using the method of koehn et al (2003), which is based on that of och and ney (2004). 
the baseline system we used for comparison was pharaoh (koehn et al, 2003; koehn, 2004a), as publicly distributed. 
but koehn et al (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. 
when we run a phrase-based system, pharaoh (koehn et al, 2003; koehn, 2004a), on this sentence (using the experimental setup described below), we get the following phrases with translations: (4) [aozhou] [shi] [yu] [bei han] [you] [bangjiao]1 [de shaoshu guojia zhiyi] [australia] [is] [dipl. 
to do this, we first identify initial phrase pairs using the same criterion as previous systems (och and ney, 2004; koehn et al, 2003): definition 1. 
above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (och and ney, 2004; koehn et al, 2003), or not at all (zens and ney, 2004; kumar et al, 2005). 
koehn et al (2003) mention german 〈es gibt, there is〉 as an example of a good phrase pair which is not a syntactic phrase pair, and report that favoring syntactic phrases does not improve accuracy. 
it is important because a word-aligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based machine translation (och et al, 1999), (tillmann and xia, 2003), (koehn et al, 2003)
the block set is generated using a phrase-pair selection algorithm similar to (koehn et al, 2003; al-onaizan et al, 2004), which includes some heuristic filtering to mal statement here. 
the current state of the art is represented by the so-called phrase-based translation approach (och and ney, 2004; koehn et al, 2003). 
phrase-based models (koehn et al, 2003; och and ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. 
phrase-based translation models (marcu and wong, 2002; koehn et al, 2003; och and ney, 2004), which go beyond the original ibm translation models (brown et al, 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. 
the baseline system we used for comparison was pharaoh (koehn et al, 2003; koehn, 2004), a freely available decoder for phrase-based translation models.
the inclusion of phrases longer than three words in translation resources has been avoided, as it has been shown not to have a strong impact on translation performance [koehn et al, 2003]. 
whereas language generation has benefited from syntax [wu, 1997; alshawi et al, 2000], the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor [koehn et al, 2003]. 
during the last four years, various implementations and extentions to phrase-based statistical models (marcu and wong, 2002; koehn et al, 2003; och and ney, 2004) have led to significant increases in machine translation accuracy. 
a word link extension algorithm similar to the one presented in this paper is given in (koehn et al, 2003). 
recently, various works have improved the quality of statistical machine translation systems by using phrase translation (koehn et al, 2003; marcu et al, 2002; och et al, 1999; och and ney, 2000; zens et al, 2004). 
using giza++ model 4 alignments and pharaoh (koehn et al, 2003), we achieved a bleu score of 0.3035. 
word alignment is an important component of a complete statistical machine translation pipeline (koehn et al, 2003). 
