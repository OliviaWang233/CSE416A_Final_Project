1| Two rule-based methods to convert dependency structure into phrase structure are presented by Covington and Collins et al.
1| Covington and Collins et al. use three simple rules with the help of X-bar theory and convert the dependency structure into phrase structure.
1| The dependents in the method of Collins et al.  project only one level; hence, the dependents of a head are in the same level.
2| We used the two-letter tag method proposed by Collins et al. to integrate a family of POS tags to a CPOS.
3| Previous work on treebank conversion primarily focuses on converting one grammar formalism of a treebank into another and then conducting a study on the converted treebank (Collins et al., 1999; Xia et al., 2008).
4| The coarse morphological tag provided by Czech two-letter coarse morphological tag, as described in (Collins et al., 1999), The first letter is the main POS (12 possible values), the second letter is either the morphological case field if the main POS displays case (i.e. for nouns, adjectives, pronouns, numerals and prepositions; 7 possible values), or the detailed POS if it does not (22 possible values).
5| Dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as Czech [7], Bulgarian [15], Turkish [14] and Russian [4].
6| For example, for constituent (phrase-structure) syntactic parsing [Charniak 2000; Collins 1999; Petrov et al. 2006] of Chinese, besides the most popular treebank, Penn Chinese Treebank (CTB) [Xue et al. 2002], there are other treebanks such as Tsinghua Chinese Treebank (TCT) and Peking Chinese Treebank.
6| Algorithm 2 presents the extended version of the decoding algorithm used in the Collins parser, what the algorithm needs to do is to generate edges for each span.
6| One type focuses on converting treebanks of different grammar formalisms. Collins et al. [1999] addressed constituent syntactic parsing on Czech using a treebank converted from a Prague dependency treebank.
7| Collins et al. (1999) report that an optimal tagset for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).
8| Subsequently, researchers have begun to look at both porting these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007).
9| By comparison, the case feature improved parsing for Czech (Collins et al., 1999) and the combination of the number feature for adjectives and mode feature for verbs improved results for Spanish (Cowan and Collins, 2005).
10| For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1-score.
11| Collins et al. (1999) report that an optimal tagset for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).
12| For example, Collins et al. (1999) adapted Collins' parser to Czech, a highly- inflected language.
13| Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima'an, 2008), and the effect of variable word order (Collins et al., 1999).
14| There have been some work (Collins et al., 1999b; Xia and Palmer, 2001) about converting dependency structures to phrase structures.
14| Collins et al. (1999b) proposed an algorithm to convert the Czech dependency Treebank into a phrase structure Treebank and do dependency parsing through Collins (1999a)'s model.
14| Different from tree- transformed PCFG based approach and lexicalized PCFG based approach, both of Collins et al. (1999b) and Xia and Palmer (2001) attempted to build some heuristic rules through linguistic theory, but didn't try to learn anything from Treebank.
15| Collins et al. (1999) addressed constituent syntactic parsing on Czech using a treebank converted from a Prague dependency treebank, where conversion rules derived from head-dependent pairs and heuristic rules are applied.
16| Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank (Collins et al., 1999; Forst, 2003; Wang et al., 1994; Watkinson and Manandhar, 2001).
16| Previous methods for DS to PS conversion (Collins et al., 1999; Covington, 1994; Xia and Palmer, 2001; Xia et al., 2008) often rely on pre- defined heuristic rules to eliminate converison ambiguity, e.g., minimal projection for dependents, lowest attachment position for dependents, and the selection of conversion rules that add fewer number of nodes to the converted tree.
16| Previous DS to PS conversion methods built a converted tree by iteratively attaching nodes and edges to the tree with the help of conversion rules and heuristic rules, based on current head- dependent pair from a source dependency tree and the structure of the built tree (Collins et al., 1999; Covington, 1994; Xia and Palmer, 2001; Xia et al., 2008).
16| Collins et al. (1999) performed statistical constituency parsing of Czech on a treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, e.g., one level of projection for any category, minimal projection for any dependents, and fixed position of attachment.
17| In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research.
18| Czech POS tags were obtained by the following two steps: First, we used 'feature-based tagger' included with the PDT3, and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags.
19| As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al., 1999))
20| With its 1.5 million syntactically annotated tokens, the PDT in version 2.0 forms the biggest source of Czech syntactic data. Dependency parsers, e.g. [8,9], usually use the PDT data for training their internal syntax models.
21| State-of-the-art morphological taggers can achieve accuracy rates of over 96% for English [34, 36] and 92% for highly inflected languages like Czech [20], and dependency parsers can achieve labeled accuracy rates for the same languages of 86% [31] and 80% [11], respectively.
22| It is possible to derive transformations that will convert many dependency grammars to context-free grammars, and vice versa [Collins et al. 1999].
23| Currently it is dominant that using data-driven approaches to learn parsers automatically from experience, such as probabilistic generative models [3], generative probabilistic parsing models [2] and deterministic discriminative model [7] and so on.
24| The two of them which are most conspicuous, and identified as most problematic, e.g., in (Collins et al., 1999), are rich nominal inflection (§2.1) and free word order (§2.6).
25| This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000).
26| The increasing availability of multi-format treebanks  and the automatic conversion from some formats to others, e.g. (Collins et al, 1999; Bahgat Shehata and Zanzotto, 2006), are attempts to overcome this problem.
26| For instance, the availability of parallel annotations, and among them one in Penn format, can be of some aid in investigating the irreproducibility of the state-of-the-art results on tree- banks or languages other than Penn and English, as empirically demonstrated by, e.g., (Collins et al, 1999) on Czech, (Dubey and Keller, 2003) on German, (Corazza et al, 2004) on Italian.
26| The output of this first step includes compact tags where features are expressed by short strings, like in (Collins et al, 1999).
27| Other parsers with high accuracy reported on PDT 2.0 use feature extraction as well [4,5].
27| Original MST parser uses only 2-letter tags proposed by [5].
27| We have described the feature templates and we have shown that when we are able to train over a large feature space, the addition of full 15-letters morphological tags for Czech outperforms the 2-letters tags commonly used since [5].
28| COL. Collins's parser adapted for PDT [4].
29| This two-dimensional parametrization2 was shown to improve parsing accuracy for English [4, 1] as well as other languages, e.g., German [7] Czech [5] and Chinese [2].
30| More specifically for PDT, Collins et al. (1999) relabel coordinated phrases after converting dependency structures to phrase structures.
30| The part-of-speech tagging used (both in training and testing) is the HMM tagging distributed with the treebank, with a tagging accuracy of 94.1%, and with the tagset compressed to 61 tags as in Collins et al. (1999)
31| Dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as Czech (Collins et al., 1999).
32| Collins (1997)'s parser and its reimplementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al., 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikel's web page, Arabic.
33| The usage of special knowledge bases to determine projections of categories (Xia and Palmer, 2001) would have presupposed language-dependent knowledge, so we investigated two other options: Flat rules (Collins et al., 1999) and binary rules.
33| Finally the placement of punctuation signs has a major impact on the performance of a parser (Collins et al., 1999).
33| So, Collins et al. (1999) proposed a tag classification for parsing the Czech treebank.
33| Comparable results in the literature are Schiehlen's (2004) 81.03% dependency f- score reached on the German NEGRA treebank and Collins et al.'s (1999) 80.0% labelled accuracy on the Czech PDT treebank.
34| They are referred to using the following abbreviations: McD (McDonnald's maximum spanning tree parser, [6]),6 COL (Collins's parser adapted for PDT, [7]), ZZ (rule-based dependency parser described in Section 2), AN (Holan's parser ANALOG which has no training phase and in the parsing phase it searches for the local tree configuration most similar to the training data, [5]), L2R, R2L, L23 and R32 (pushdown parsers introduced in Section 3).
35| We thus augment the syntactic type of conjuncts to include the type of the conjoined constituents by using the syntactic type of the head child. This is similar to what was done for Czech by Collins et al. [1].
36| The results so far mainly come from studies where a parser originally developed for English, such as the Collins parser [1], is applied to a new language, which often leads to a significant decrease in the measured accuracy [2,3,4,5,6].
37| The remaining set of projected trees becomes the treebank that will be used to train a new dependency parser — we conduct our experiments using a version of the Collins parser that has been adapted for dependency treebanks (Collins et al. 1999).
38| In particular, we used the method of Collins et al. (1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features.
38| The Czech parser of Collins et al. (1999) was run on a different data set and most other dependency parsers are evaluated using English.
39| It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003).
40| Interest in parsing models for languages other than English has been growing, starting with work on Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003).
40| Bigram Model This model, inspired by the approach of Collins et al. (1999) for parsing the Prague Dependency Treebank, builds on Collins' Model 2 by implementing a 1st order Markov assumption for the generation of sister non-terminals.
40| This is consistent with the findings of Collins et al. (1999) for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where Charniak (2000) reports an increase in F-score of approximately 0.3%.
41| Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005).
41| To reduce sparseness, our features rely only on the reduced POS tag set from Collins et al. (1999).
41| COLL1999: The projective lexicalized phrase-structure parser of Collins et al. (1999).
41| Furthermore, we can also see that the MST parsers perform favorably compared to the more powerful lexicalized phrase-structure parsers, such as those presented by Collins et al. (1999) and Zeman (2004) that use expensive O n5 parsing algorithms.
41| We should note that the results in Collins et al. (1999) are different then reported here due to different training and testing data sets.
42| For instance, work has been done in Chinese using the Penn Chinese Tree- bank (Levy and Manning, 2003; Chiang and Bikel, 2002), in Czech using the Prague Dependency Tree- bank (Collins et al., 1999), in French using the French Treebank (Arun and Keller, 2005), in German using the Negra Treebank (Dubey, 2005; Dubey and Keller, 2003), and in Spanish using the UAM Spanish Treebank (Moreno et al., 2000).
42| The authors in (Collins et al., 1999) describe an approach that gives 80% accuracy in recovering unlabeled dependencies in Czech.1
43| Uses a probabilistic context-free grammar, home in constituency- based structures. Described in (Haji et al., 1998; Collins et al., 1999).
44| Statistical parsing models have been shown to be successful in recovering labeled constituencies (Collins, 2003; Charniak and Johnson, 2005; Roark and Collins, 2004) and have also been shown to be adequate in recovering dependency relationships (Collins et al., 1999;
44| A pragmatic justification for using constituency- based parsers in order to predict dependency structures is that currently the best Czech dependency- tree parser is a constituency-based parser (Collins et al., 1999; Zeman, 2004).
44| In an attempt to extend a constituency-based parsing model to train on dependency trees, Collins transforms the PDT dependency trees into constituency trees (Collins et al., 1999).
44| The trees are then transformed into Penn Treebank style constituencies using the technique described in (Collins et al., 1999).
44| Although the results presented in (Collins et al., 1999) used the reordering technique, we have experimented with his parser using the governor-raising technique and observe an increase in dependency accuracy.
44| a subset of the morphological tag as described in (Collins et al., 1999)
45| Second, Czech exhibits a “relatively free word order” [7].
46| From the resulting (noisy) dependency treebank, a dependency parser is trained using the techniques of (Collins, 1999).
47| Unlabeled attachment score (UAS): The proportion of words that are assigned the correct head (or no head if the word is a root) (Eisner, 1996; Collins et al., 1999).
48| This development has fueled interest in proting the parsing technologies developed for English and the Penn treebank format to other languages and representation formats(Collins et al., 1999; Debey and Leller, 2003)
49| The number of recovered dependencies gives an estimate of the probability that a dependency will be correctly identified by the LG parser (this criterion is also employed by, e.g., Collins et al. (1999)).
50| Here 80% accuracy for unlabelled dependencies have been achieved(Collins et al., 1999)
51| Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000).
51| The work by Collins et al. (1999) and Bikel and Chiang (2000) has demonstrated the applicability of the Collins (1997) model for Czech and Chinese.
51| However, the learning curve for Negra (see Figure 1) indicates that the performance of the Collins (1997) model is stable, even for small training sets. Collins et al. (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages.
52| We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)).
53| However, such constructions prove to be difficult for stochastic parsers (Collins et al., 1999) and they either avoid tackling the problem (Charniak, 2000; Bod, 2003) or only deal with a subset of the problematic cases (Collins, 1997).
54| Collins et al. (1999) describe how the models in the current article were applied to parsing Czech.
55| Dependency-based statistical language modeling and parsing have also become quite popular in statistical natural language processing (Lafferty, Sleator, and Temperley 1992; Eisner 1996; Chelba et al. 1997; Collins 1996; Collins et al. 1999).
56| The ATSs produced by Collins' dependency parser(Collins et al., 1999), which yields the ATSs, are manually edited; instructuions for the editing have been formulated and apporximately 100000 sentences have been annotated at the analytical level.
57| Algorithm 2, as adopted by Collins and his colleagues [2] when they converted the Czech dependency Treebank [6] into a phrase- structure Treebank, produces phrase structures that are as flat as possible.
58| the sentences from CNC are preprocessed by a dependency-based modification of Collins et al.'s (1999) automatic parser (with a success rate of about 80%), followed by a manual tagging procedure that is supported by a special user-friendly software tool that enables the annotators to work with (i.e. modify) the automatically derived graphic representations of the trees;
59| For a detailed descriptions see Haji6 (1998), Hladkfi (2000) and Collins, Haji6, Ram~haw, Tillmann (1999).
59| We use the statistical Collins's parser to create the structure of the tree and then a statistical procedure to assign words their syntactic functions.