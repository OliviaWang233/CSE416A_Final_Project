{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fab27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy\n",
    "from networkx.algorithms.community import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f3726",
   "metadata": {},
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a31465e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'N03-1017.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hj/cznj72454zx5dsbnb7_rbmth0000gn/T/ipykernel_6021/2354481498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mG_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N03-1017.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'N03-1017.txt'"
     ]
    }
   ],
   "source": [
    "# add nodes\n",
    " \n",
    "G = nx.Graph()\n",
    "G_copy = nx.Graph()\n",
    "sentences = []\n",
    "with open('N03-1017.csv','r') as csvfile:\n",
    "    file = csv.reader(csvfile, delimiter=\"|\")\n",
    "    for row in file:\n",
    "        sentences.append(row[1])\n",
    "        #G.add_node(row[1], article_index = row[0])\n",
    "        G.add_node(row[0])\n",
    "        G_copy.add_node(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b424195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 91 nodes and 1210 edges\n"
     ]
    }
   ],
   "source": [
    "'''''''''running time of this cell is about one minute'''''''''\n",
    "\n",
    "# add edges with weight \n",
    "# rule: if the two sentences' similarity > N, these two nodes have edge\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "for i in range(len(sentences)-1):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        doc1 = nlp(sentences[i])\n",
    "        doc2 = nlp(sentences[j])\n",
    "        similarity = ((doc1.similarity(doc2))-0.5)*200 # 0 - 100\n",
    "        if similarity > 80:\n",
    "            G.add_edge(sentences[i], sentences[j], weight=similarity)\n",
    "            G_copy.add_edge(sentences[i], sentences[j], weight=similarity)\n",
    "print(G)\n",
    "nx.write_gexf(G, \"G.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4064c",
   "metadata": {},
   "source": [
    "# Current Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf68e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_number = 1\n",
      "# nodes in connected components are [86]\n"
     ]
    }
   ],
   "source": [
    "CC_number = nx.number_connected_components(G)\n",
    "print('CC_number = ' + str(CC_number))\n",
    "number_of_nodes_in_CC = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "print('# nodes in connected components are ' + str(number_of_nodes_in_CC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "147b3aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'degree_sequence = nx.degree_histogram(G)\\nprint(degree_sequence)\\nx = np.arange(len(degree_sequence))\\nfig, ax = plt.subplots()\\nax.scatter(x, degree_sequence)'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''degree_sequence = nx.degree_histogram(G)\n",
    "print(degree_sequence)\n",
    "x = np.arange(len(degree_sequence))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, degree_sequence)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9234ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 86 nodes and 1210 edges\n"
     ]
    }
   ],
   "source": [
    "#remove the isolated nodes\n",
    "CC = [c for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "for i in range(CC_number):\n",
    "    if number_of_nodes_in_CC[i]==1:\n",
    "        G.remove_nodes_from(CC[i])\n",
    "\n",
    "print(G)\n",
    "nx.write_gexf(G, \"G_remove_isolated_nodes.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e84f1",
   "metadata": {},
   "source": [
    "# Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727cb51",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcf527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_communities = 4\n",
    "c = greedy_modularity_communities(G, weight = 'weight')\n",
    "all_S = []\n",
    "for i in range(number_of_communities):\n",
    "    all_S.append(list(c[i]))\n",
    "\n",
    "for partition_index in range(len(all_S)):\n",
    "    G.add_nodes_from(all_S[partition_index], partition = partition_index)\n",
    "\n",
    "\n",
    "#nx.write_gexf(G, \"G_partition.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d3fa6",
   "metadata": {},
   "source": [
    "### Extract the salient sentence in each community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e7715",
   "metadata": {},
   "source": [
    "##### Eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7dc5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_sentences_in_each_community = []\n",
    "for h in range(number_of_communities):\n",
    "    S = nx.Graph()\n",
    "    S_each = all_S[h]\n",
    "    S.add_nodes_from(S_each)\n",
    "    for i in range(len(S_each)-1):\n",
    "        for j in range(i+1, len(S_each)):\n",
    "            doc1 = nlp(S_each[i])\n",
    "            doc2 = nlp(S_each[j])\n",
    "            similarity = ((doc1.similarity(doc2))-0.5)*200\n",
    "            if similarity>80:\n",
    "                S.add_edge(S_each[i], S_each[j], weight=similarity, distance = 100-similarity)\n",
    "                \n",
    "    S_rank = nx.eigenvector_centrality(S, weight='weight')\n",
    "    #S_rank = nx.closeness_centrality(S, distance = 'distance')\n",
    "    #S_rank = nx.degree_centrality(S)\n",
    "    #S_rank = nx.betweenness_centrality(S,weight='weight')\n",
    "    S_rank = sorted(S_rank.items(), key=lambda item: item[1], reverse=True)\n",
    "    for item in S_rank:\n",
    "        G.add_node(item[0], centrality = item[1])\n",
    "\n",
    "    \n",
    "    top_sentences_in_each_community.append(S_rank[0])\n",
    "\n",
    "#nx.write_gexf(G, \"G_eigenvector.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c2917ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\" the sentences from CNC are preprocessed by a dependency-based modification of Collins et al.'s (1999) automatic parser (with a success rate of about 80%), followed by a manual tagging procedure that is supported by a special user-friendly software tool that enables the annotators to work with (i.e. modify) the automatically derived graphic representations of the trees;\", 0.22381541424078108)\n",
      "(' Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank (Collins et al., 1999; Forst, 2003; Wang et al., 1994; Watkinson and Manandhar, 2001).', 0.2582364682759764)\n",
      "(' Dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as Czech [7], Bulgarian [15], Turkish [14] and Russian [4].', 0.5828794132587916)\n",
      "(' The authors in (Collins et al., 1999) describe an approach that gives 80% accuracy in recovering unlabeled dependencies in Czech.1', 0.5285590881136547)\n"
     ]
    }
   ],
   "source": [
    "# Select the most important information (centrality value < 1)\n",
    "\n",
    "for sentence in top_sentences_in_each_community:\n",
    "    if sentence[1] <1 :\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81673c98",
   "metadata": {},
   "source": [
    "##### Beteenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "58e65f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences_in_each_community = []\n",
    "\n",
    "for h in range(number_of_communities):\n",
    "    S = nx.Graph()\n",
    "    S_each = all_S[h]\n",
    "    S.add_nodes_from(S_each)\n",
    "    for i in range(len(S_each)-1):\n",
    "        for j in range(i+1, len(S_each)):\n",
    "            doc1 = nlp(S_each[i])\n",
    "            doc2 = nlp(S_each[j])\n",
    "            similarity = ((doc1.similarity(doc2))-0.5)*200\n",
    "            if similarity>80:\n",
    "                S.add_edge(S_each[i], S_each[j], weight=similarity, distance = 100-similarity)\n",
    "    #S_rank = nx.eigenvector_centrality(S, weight='weight')\n",
    "    #S_rank = nx.closeness_centrality(S, distance = 'distance')\n",
    "    #S_rank = nx.degree_centrality(S)\n",
    "    S_rank = nx.betweenness_centrality(S,weight='weight')\n",
    "    S_rank = sorted(S_rank.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_sentences_in_each_community.append(S_rank[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "73d4fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\" Collins et al. (1999b) proposed an algorithm to convert the Czech dependency Treebank into a phrase structure Treebank and do dependency parsing through Collins (1999a)'s model.\", 0.052631578947368425)\n",
      "(' It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', 0.06854838709677419)\n",
      "(' Dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as Czech [7], Bulgarian [15], Turkish [14] and Russian [4].', 0.16666666666666666)\n"
     ]
    }
   ],
   "source": [
    "# Select the most important information (centrality value > 0)\n",
    "\n",
    "for sentence in top_sentences_in_each_community:\n",
    "    if sentence[1] >0 :\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24addabc",
   "metadata": {},
   "source": [
    "##### Closeness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "90bce424",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences_in_each_community = []\n",
    "\n",
    "for h in range(number_of_communities):\n",
    "    S = nx.Graph()\n",
    "    S_each = all_S[h]\n",
    "    S.add_nodes_from(S_each)\n",
    "    for i in range(len(S_each)-1):\n",
    "        for j in range(i+1, len(S_each)):\n",
    "            doc1 = nlp(S_each[i])\n",
    "            doc2 = nlp(S_each[j])\n",
    "            similarity = ((doc1.similarity(doc2))-0.5)*200\n",
    "            if similarity>80:\n",
    "                S.add_edge(S_each[i], S_each[j], weight=similarity, distance = 100-similarity)\n",
    "    #S_rank = nx.eigenvector_centrality(S, weight='weight')\n",
    "    S_rank = nx.closeness_centrality(S, distance = 'distance')\n",
    "    #S_rank = nx.degree_centrality(S)\n",
    "    #S_rank = nx.betweenness_centrality(S,weight='weight')\n",
    "    S_rank = sorted(S_rank.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_sentences_in_each_community.append(S_rank[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "615eeac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The remaining set of projected trees becomes the treebank that will be used to train a new dependency parser â€” we conduct our experiments using a version of the Collins parser that has been adapted for dependency treebanks (Collins et al. 1999).', 0.05811179519093787)\n",
      "(' Interest in parsing models for languages other than English has been growing, starting with work on Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003).', 0.060547669912540354)\n",
      "(' Dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as Czech [7], Bulgarian [15], Turkish [14] and Russian [4].', 0.0646647384264534)\n",
      "(' This is consistent with the findings of Collins et al. (1999) for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where Charniak (2000) reports an increase in F-score of approximately 0.3%.', 0.07873607542856635)\n"
     ]
    }
   ],
   "source": [
    "# Select the most important information (centrality value > 0)\n",
    "\n",
    "for sentence in top_sentences_in_each_community:\n",
    "    if sentence[1] >0 :\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa99bc",
   "metadata": {},
   "source": [
    "##### Degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3ac99cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences_in_each_community = []\n",
    "\n",
    "for h in range(number_of_communities):\n",
    "    S = nx.Graph()\n",
    "    S_each = all_S[h]\n",
    "    S.add_nodes_from(S_each)\n",
    "    for i in range(len(S_each)-1):\n",
    "        for j in range(i+1, len(S_each)):\n",
    "            doc1 = nlp(S_each[i])\n",
    "            doc2 = nlp(S_each[j])\n",
    "            similarity = ((doc1.similarity(doc2))-0.5)*200\n",
    "            if similarity>80:\n",
    "                S.add_edge(S_each[i], S_each[j], weight=similarity, distance = 100-similarity)\n",
    "    #S_rank = nx.eigenvector_centrality(S, weight='weight')\n",
    "    #S_rank = nx.closeness_centrality(S, distance = 'distance')\n",
    "    S_rank = nx.degree_centrality(S)\n",
    "    #S_rank = nx.betweenness_centrality(S,weight='weight')\n",
    "    S_rank = sorted(S_rank.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_sentences_in_each_community.append(S_rank[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a77242b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\" the sentences from CNC are preprocessed by a dependency-based modification of Collins et al.'s (1999) automatic parser (with a success rate of about 80%), followed by a manual tagging procedure that is supported by a special user-friendly software tool that enables the annotators to work with (i.e. modify) the automatically derived graphic representations of the trees;\", 0.8421052631578947)\n",
      "(' Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank (Collins et al., 1999; Forst, 2003; Wang et al., 1994; Watkinson and Manandhar, 2001).', 0.84375)\n"
     ]
    }
   ],
   "source": [
    "# Select the most important information (centrality value < 1)\n",
    "\n",
    "for sentence in top_sentences_in_each_community:\n",
    "    if sentence[1] <1 :\n",
    "        print(sentence)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
