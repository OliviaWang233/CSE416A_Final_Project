{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30fab27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f3726",
   "metadata": {},
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a31465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nodes\n",
    " \n",
    "G = nx.Graph()\n",
    "G_copy = nx.Graph()\n",
    "sentences = []\n",
    "with open('data.csv','r') as csvfile:\n",
    "    file = csv.reader(csvfile, delimiter=\"|\")\n",
    "    for row in file:\n",
    "        sentences.append(row[1])\n",
    "        G.add_node(row[1], article_index = row[0])\n",
    "        G_copy.add_node(row[1], article_index = row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b424195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 91 nodes and 356 edges\n"
     ]
    }
   ],
   "source": [
    "'''''''''running time of this cell is about one minute'''''''''\n",
    "\n",
    "# add edges with weight \n",
    "# rule: if the two sentences' similarity > N, these two nodes have edge\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "for i in range(len(sentences)-1):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        doc1 = nlp(sentences[i])\n",
    "        doc2 = nlp(sentences[j])\n",
    "        similarity = ((doc1.similarity(doc2))-0.5)*200 # 0 - 100\n",
    "        if similarity > 85:\n",
    "            G.add_edge(sentences[i], sentences[j], weight=similarity)\n",
    "            G_copy.add_edge(sentences[i], sentences[j], weight=similarity)\n",
    "print(G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4064c",
   "metadata": {},
   "source": [
    "# Current Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf68e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_number = 13\n",
      "# nodes in connected components are [78, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "CC_number = nx.number_connected_components(G)\n",
    "print('CC_number = ' + str(CC_number))\n",
    "number_of_nodes_in_CC = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "print('# nodes in connected components are ' + str(number_of_nodes_in_CC))\n",
    "\n",
    "#cc = nx.connected_components(G)\n",
    "#for i in cc:\n",
    "#    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "147b3aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'degree_sequence = nx.degree_histogram(G)\\nprint(degree_sequence)\\nx = np.arange(len(degree_sequence))\\nfig, ax = plt.subplots()\\nax.scatter(x, degree_sequence)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''degree_sequence = nx.degree_histogram(G)\n",
    "print(degree_sequence)\n",
    "x = np.arange(len(degree_sequence))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, degree_sequence)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9234ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 80 nodes and 356 edges\n"
     ]
    }
   ],
   "source": [
    "#remove the isolated nodes\n",
    "CC = [c for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "for i in range(CC_number):\n",
    "    if number_of_nodes_in_CC[i]==1:\n",
    "        G.remove_nodes_from(CC[i])\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e84f1",
   "metadata": {},
   "source": [
    "# Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727cb51",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcf527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_communities = 15\n",
    "c = greedy_modularity_communities(G, weight = 'weight',n_communities = number_of_communities)\n",
    "all_S = []\n",
    "for i in range(number_of_communities):\n",
    "    all_S.append(list(c[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d3fa6",
   "metadata": {},
   "source": [
    "### Extract the salient sentence in each community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7dc5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_sentences_in_each_community = []\n",
    "\n",
    "for h in range(number_of_communities):\n",
    "    S = nx.Graph()\n",
    "    S_each = all_S[h]\n",
    "    S.add_nodes_from(S_each)\n",
    "    for i in range(len(S_each)-1):\n",
    "        for j in range(i+1, len(S_each)):\n",
    "            doc1 = nlp(S_each[i])\n",
    "            doc2 = nlp(S_each[j])\n",
    "            similarity = ((doc1.similarity(doc2))-0.5)*200\n",
    "            if similarity>80:\n",
    "                S.add_edge(S_each[i], S_each[j], weight=similarity, distance = 100-similarity)\n",
    "    #S_rank = nx.eigenvector_centrality(S, weight='weight')\n",
    "    #S_rank = nx.closeness_centrality(S, distance = 'distance')\n",
    "    #S_rank = nx.degree_centrality(S)\n",
    "    S_rank = nx.betweenness_centrality(S,weight='weight')\n",
    "    S_rank = sorted(S_rank.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_sentences_in_each_community.append(S_rank[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd7b6c",
   "metadata": {},
   "source": [
    "### Select the meaningful sentencers (centrality value > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c2917ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The part-of-speech tagging used (both in training and testing) is the HMM tagging distributed with the treebank, with a tagging accuracy of 94.1%, and with the tagset compressed to 61 tags as in Collins et al. (1999)\n",
      " Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005).\n",
      " Collins et al. (1999b) proposed an algorithm to convert the Czech dependency Treebank into a phrase structure Treebank and do dependency parsing through Collins (1999a)'s model.\n",
      " So, Collins et al. (1999) proposed a tag classification for parsing the Czech treebank.\n"
     ]
    }
   ],
   "source": [
    "for sentence in top_sentences_in_each_community:\n",
    "    if sentence[1] >0:\n",
    "        print(sentence[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
